{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üõ≥Ô∏è **Titanic - Machine Learning from Disaster**   \nThis project aims to predict which passengers survived the Titanic shipwreck using historical data.  \nIt demonstrates an end-to-end machine learning workflow including data cleaning, feature engineering, model training, and evaluation.\n","metadata":{}},{"cell_type":"markdown","source":"We load the Titanic dataset provided by Kaggle and begin understanding the structure, types of variables, and initial issues like missing values.  \n\nKey columns to explore:  \n- Categorical: Sex, Embarked, Pclass, Cabin  \n- Numerical: Age, Fare, SibSp, Parch\n","metadata":{}},{"cell_type":"code","source":"# Importing Relevant Libraries \n\n# Data Preparation\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\n\n# Modelling & Feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import Pipeline\n\n# Evaluation & Metrics\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay,\n    precision_score, recall_score, f1_score\n)\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:50.382357Z","iopub.execute_input":"2025-06-29T08:36:50.382643Z","iopub.status.idle":"2025-06-29T08:36:50.388022Z","shell.execute_reply.started":"2025-06-29T08:36:50.382621Z","shell.execute_reply":"2025-06-29T08:36:50.387083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:50.389603Z","iopub.execute_input":"2025-06-29T08:36:50.390428Z","iopub.status.idle":"2025-06-29T08:36:50.429112Z","shell.execute_reply.started":"2025-06-29T08:36:50.390407Z","shell.execute_reply":"2025-06-29T08:36:50.428200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"overall = pd.concat([train_data,test_data])\noverall.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:50.429856Z","iopub.execute_input":"2025-06-29T08:36:50.430087Z","iopub.status.idle":"2025-06-29T08:36:50.448169Z","shell.execute_reply.started":"2025-06-29T08:36:50.430070Z","shell.execute_reply":"2025-06-29T08:36:50.447439Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä 2. Exploratory Data Analysis (EDA)\nBefore modeling, we explore:\n- Missing value patterns\n- Survival rates across key groups (e.g., Sex, Pclass)\n- Age and Fare distributions\n- Correlation among features\n\nInsights will guide our feature engineering.\n","metadata":{}},{"cell_type":"code","source":"plt.rc('font',size=14)\nplt.rc('axes',labelsize=14, titlesize=14)\nplt.rc('legend',fontsize=14)\nplt.rc('xtick',labelsize=10)\nplt.rc('ytick',labelsize=10)\n\ntrain_data.hist(bins=50,figsize=(12,8))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:50.449051Z","iopub.execute_input":"2025-06-29T08:36:50.449302Z","iopub.status.idle":"2025-06-29T08:36:51.784274Z","shell.execute_reply.started":"2025-06-29T08:36:50.449284Z","shell.execute_reply":"2025-06-29T08:36:51.783500Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checking Count of Embarked values to estimate for filling null values","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=overall,x='Embarked')\nplt.title(\"Passenger count by Embarked Port\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:51.786601Z","iopub.execute_input":"2025-06-29T08:36:51.786850Z","iopub.status.idle":"2025-06-29T08:36:51.940200Z","shell.execute_reply.started":"2025-06-29T08:36:51.786830Z","shell.execute_reply":"2025-06-29T08:36:51.939396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n#Sex vs Survived\nsns.barplot(data=overall, x='Sex', y='Survived', ax=axes[0], palette='pastel')\naxes[0].set_title('Survival Rate by Sex')\naxes[0].set_ylabel('Survival Rate')\n\n#Pclass vs Survived\nsns.barplot(data=overall, x='Pclass', y='Survived', ax=axes[1], palette='muted')\naxes[1].set_title('Survival Rate by Pclass')\naxes[1].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:51.941098Z","iopub.execute_input":"2025-06-29T08:36:51.941386Z","iopub.status.idle":"2025-06-29T08:36:52.498402Z","shell.execute_reply.started":"2025-06-29T08:36:51.941366Z","shell.execute_reply":"2025-06-29T08:36:52.497676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Filling null values with M and checking Cabin values distribution","metadata":{}},{"cell_type":"code","source":"overall['Cabin'] = overall['Cabin'].apply(lambda x : str(x)[0] if pd.notnull(x) else 'M')\n\nsns.countplot(data=overall,x='Cabin')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.499099Z","iopub.execute_input":"2025-06-29T08:36:52.499373Z","iopub.status.idle":"2025-06-29T08:36:52.653609Z","shell.execute_reply.started":"2025-06-29T08:36:52.499353Z","shell.execute_reply":"2025-06-29T08:36:52.652901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß± 3. Data Cleaning\nSteps performed:\n\nThere are null values in Age, Cabin, Embarked and Fare.  \n1. Null Age is filled by median.  \n2. Embarked is filled by 'S' as only 2 values is missing and it is the mode.  \n3. Cabins majority value is null. To handle this I am replacing null values with 'Missing'.  \n4. Fare null value is filled with mean as only 1 value is missing. Also, there is long tail (skewness) in Fare value hence I am taking log1p as Fare values are small.","metadata":{}},{"cell_type":"code","source":"train_data['Fare_to_Class'] = train_data['Fare'] / train_data['Pclass']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.654381Z","iopub.execute_input":"2025-06-29T08:36:52.654584Z","iopub.status.idle":"2025-06-29T08:36:52.659666Z","shell.execute_reply.started":"2025-06-29T08:36:52.654569Z","shell.execute_reply":"2025-06-29T08:36:52.659094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ncorr_matrix = train_data.corr(numeric_only=True)\ncorr_matrix['Survived'].sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.660327Z","iopub.execute_input":"2025-06-29T08:36:52.660510Z","iopub.status.idle":"2025-06-29T08:36:52.679372Z","shell.execute_reply.started":"2025-06-29T08:36:52.660495Z","shell.execute_reply":"2025-06-29T08:36:52.678676Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There is very less correlation between Passenger ID and Survival and it makes sense as it is just sequence of numbers.  \nFare_to_class have a better correlation than Fare.","metadata":{}},{"cell_type":"markdown","source":"# üß† 4. Feature Engineering\nDefining a function to preprocess all the data.\n1. Extracted 'Title' from Name. Divided this into [Officers - Working on Deck, Nobles - High Worth People, Married Female - Usually preferred during Evacuation, Common Male and Unmarried Female.\n2. Extracted Ticket Number and Ticket Prefix from Ticket.\n3. Created Age Band each of 16 years.\n4. Created Total Family members count inclusing self (SplSb + Parch + 1)\n5. Also added indicator column for passenger: is alone or not (IsAlone)\n6. Dropped less contributing columns","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    \n    def ticket_split(x):\n        return x.split(\" \")[-1]\n\n    def ticket_pref(x):\n        pref = x.split(\" \")\n        if len(pref) == 1:\n            return \"NONE\"\n        return '_'.join(pref[0:-1])\n\n    def Title(df):\n        df['Title'] = df['Name'].str.extract(r'^[^,]+,\\s*(.*?)\\.')[0]\n        df['Title'] = df['Title'].replace(['Ms', 'Mlle', 'Miss'],'Unmarried Female')\n        df['Title'] = df['Title'].replace(['Mr','Master'],'Common Male')\n        df['Title'] = df['Title'].replace(['Mrs', 'Mme'],'Married Female')\n        df['Title'] = df['Title'].replace(['Col', 'Capt', 'Major', 'Rev', 'Dr'],'Officers')\n        df['Title'] = df['Title'].replace(['Don', 'Jonkheer', 'the Countess','Dona', 'Sir', 'Lady'],'Nobles')\n        return df\n\n    df = Title(df)\n\n    df['Title'] = df['Title'].str[0]\n    \n    def name(x):\n        return \" \".join(v.strip(\",()[].\\\"'\") for v in x.split(\" \"))\n\n    df['Name'] = df['Name'].apply(name)\n    df['Ticket_Number'] = df['Ticket'].apply(ticket_split)\n    df['Ticket_Prefix'] = df['Ticket'].apply(ticket_pref)\n\n    df['Age'] = df['Age'].fillna(value=df['Age'].median())\n    df['Embarked'] = df['Embarked'].fillna('S')\n    \n    bins = [0,16,32,48,64,float('inf')]\n    labels = [0,1,2,3,4]\n    df['Age_Band'] = pd.cut(df['Age'],bins=bins,labels=labels,right=True)\n\n    df['Cabin'] = df['Cabin'].fillna('Missing')\n    df['Cabin'] = df['Cabin'].str[0]\n\n    df['Fare'] = df['Fare'].fillna(value=df['Fare'].mean())\n    df['Fare'] = np.log1p(df['Fare'])\n\n    df['Fare_to_Class'] = df['Fare'] / df['Pclass']\n    \n    df['Fam_Count'] = df['SibSp']+df['Parch']+1\n\n    df['IsAlone'] = df['Fam_Count'].apply(lambda x: 1 if x==1 else 0)\n    \n    df = df.drop(['PassengerId','Name','Ticket','Age','Pclass','Fare'],axis=1)\n    \n    return df\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.680151Z","iopub.execute_input":"2025-06-29T08:36:52.680384Z","iopub.status.idle":"2025-06-29T08:36:52.694446Z","shell.execute_reply.started":"2025-06-29T08:36:52.680367Z","shell.execute_reply":"2025-06-29T08:36:52.693629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessed_train_df = preprocess(train_data)\npreprocessed_test_df = preprocess(test_data)\npreprocessed_train_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.695174Z","iopub.execute_input":"2025-06-29T08:36:52.695388Z","iopub.status.idle":"2025-06-29T08:36:52.762337Z","shell.execute_reply.started":"2025-06-29T08:36:52.695370Z","shell.execute_reply":"2025-06-29T08:36:52.761646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat = preprocessed_train_df.select_dtypes(\"object\").columns\nnum = ['Pclass','SibSp','Parch','Fare','Age_band','Fam_Count','IsAlone']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.763104Z","iopub.execute_input":"2025-06-29T08:36:52.763325Z","iopub.status.idle":"2025-06-29T08:36:52.768700Z","shell.execute_reply.started":"2025-06-29T08:36:52.763307Z","shell.execute_reply":"2025-06-29T08:36:52.767979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train = preprocessed_train_df['Survived']\nX_train = preprocessed_train_df.drop(['Survived'],axis=1)\nX_test = preprocessed_test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.771687Z","iopub.execute_input":"2025-06-29T08:36:52.771915Z","iopub.status.idle":"2025-06-29T08:36:52.789286Z","shell.execute_reply.started":"2025-06-29T08:36:52.771898Z","shell.execute_reply":"2025-06-29T08:36:52.788397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating Train and Test Split for validation","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.790152Z","iopub.execute_input":"2025-06-29T08:36:52.790373Z","iopub.status.idle":"2025-06-29T08:36:52.806456Z","shell.execute_reply.started":"2025-06-29T08:36:52.790356Z","shell.execute_reply":"2025-06-29T08:36:52.805583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I am using column Transformer to tranform Categorical columns using OneHotEncoder and Numericals using StandardScaler.  \nAdding SelectFromModel as it selects the features with decent feature_importances_  \nUsing RandomForestClassifier for this Classification problem (It works well as it reduces overfitting compared to single decision tree, provide feature_importances_ to study impact of features on label)","metadata":{}},{"cell_type":"code","source":"cat_encoder = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\nrf_clf = RandomForestClassifier(random_state=42)\nfeature_selector = SelectFromModel(RandomForestClassifier(random_state=42))\ncol_transform = make_column_transformer((cat_encoder,cat),remainder = StandardScaler())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.807368Z","iopub.execute_input":"2025-06-29T08:36:52.807638Z","iopub.status.idle":"2025-06-29T08:36:52.820176Z","shell.execute_reply.started":"2025-06-29T08:36:52.807614Z","shell.execute_reply":"2025-06-29T08:36:52.819316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipeline = Pipeline([('col',col_transform),('clf',rf_clf)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.820909Z","iopub.execute_input":"2025-06-29T08:36:52.821200Z","iopub.status.idle":"2025-06-29T08:36:52.837525Z","shell.execute_reply.started":"2025-06-29T08:36:52.821176Z","shell.execute_reply":"2025-06-29T08:36:52.836456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training on Processed Data","metadata":{}},{"cell_type":"code","source":"pipeline.fit(X_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:52.838399Z","iopub.execute_input":"2025-06-29T08:36:52.838664Z","iopub.status.idle":"2025-06-29T08:36:53.196826Z","shell.execute_reply.started":"2025-06-29T08:36:52.838644Z","shell.execute_reply":"2025-06-29T08:36:53.196122Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I used GridSearchCV to tune hyperparameters like n_estimators, max_depth, min_sample_split, max_features).","metadata":{}},{"cell_type":"code","source":"param_grid = {'clf__n_estimators':[64, 128, 200],\n             'clf__max_features':[2, 4, 'sqrt'],\n             'clf__max_depth':[2, 3, 4],\n             'clf__min_samples_split':[2, 3,4],\n             'clf__min_samples_leaf': [2,3,4],\n             'clf__max_samples':[0.8, 1]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:53.197718Z","iopub.execute_input":"2025-06-29T08:36:53.198451Z","iopub.status.idle":"2025-06-29T08:36:53.202428Z","shell.execute_reply.started":"2025-06-29T08:36:53.198423Z","shell.execute_reply":"2025-06-29T08:36:53.201591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RF_grid = GridSearchCV(estimator=pipeline,param_grid=param_grid,scoring='recall',\n                       n_jobs=-1,verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:53.203429Z","iopub.execute_input":"2025-06-29T08:36:53.203683Z","iopub.status.idle":"2025-06-29T08:36:53.217575Z","shell.execute_reply.started":"2025-06-29T08:36:53.203659Z","shell.execute_reply":"2025-06-29T08:36:53.216900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RF_grid.fit(X_train,y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:36:53.218332Z","iopub.execute_input":"2025-06-29T08:36:53.219019Z","iopub.status.idle":"2025-06-29T08:40:27.418430Z","shell.execute_reply.started":"2025-06-29T08:36:53.218993Z","shell.execute_reply":"2025-06-29T08:40:27.417609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RF_grid.best_params_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.419266Z","iopub.execute_input":"2025-06-29T08:40:27.419508Z","iopub.status.idle":"2025-06-29T08:40:27.424633Z","shell.execute_reply.started":"2025-06-29T08:40:27.419490Z","shell.execute_reply":"2025-06-29T08:40:27.423646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predict_val = RF_grid.predict(X_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.425522Z","iopub.execute_input":"2025-06-29T08:40:27.425758Z","iopub.status.idle":"2025-06-29T08:40:27.457535Z","shell.execute_reply.started":"2025-06-29T08:40:27.425735Z","shell.execute_reply":"2025-06-29T08:40:27.456888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = pd.DataFrame(pipeline['clf'].feature_importances_ , columns=['Imp'] , index = pipeline['col'].get_feature_names_out())\nfeatures.sort_values(by='Imp',ascending=False).head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.458354Z","iopub.execute_input":"2025-06-29T08:40:27.458628Z","iopub.status.idle":"2025-06-29T08:40:27.484596Z","shell.execute_reply.started":"2025-06-29T08:40:27.458605Z","shell.execute_reply":"2025-06-29T08:40:27.483982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Checked Precision, Recall and F1 score on validation set.","metadata":{}},{"cell_type":"code","source":"precision = precision_score(y_val,predict_val)\nprint(f\"Precision : {precision:.4f}\")\n\nrecall = recall_score(y_val,predict_val)\nprint(f\"Recall : {recall:.4f}\")\n\nf1 = f1_score(y_val,predict_val)\nprint(f\"F1 : {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.485370Z","iopub.execute_input":"2025-06-29T08:40:27.485589Z","iopub.status.idle":"2025-06-29T08:40:27.497516Z","shell.execute_reply.started":"2025-06-29T08:40:27.485568Z","shell.execute_reply":"2025-06-29T08:40:27.496761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_val,predict_val)\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.498333Z","iopub.execute_input":"2025-06-29T08:40:27.498637Z","iopub.status.idle":"2025-06-29T08:40:27.685197Z","shell.execute_reply.started":"2025-06-29T08:40:27.498619Z","shell.execute_reply":"2025-06-29T08:40:27.684380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The confusion matrix reveals how well the model is distinguishing between survivors and non-survivors. Out of the total, 97 survivors were correctly identified (True Positives), while only 8 were missed (False Negatives). However, 36 non-survivors were mistakenly classified as survivors (False Positives), indicating a slight bias toward predicting survival. The model also correctly flagged 38 individuals as non-survivors (True Negatives). Overall, the model favors recall for survival, potentially prioritizing fewer missed survivors at the cost of more false alarms.\n\nTo improve the model‚Äôs ability to identify survivors, exploring strategies such as tuning classification thresholds, experimenting with different models or ensemble methods may help distinguish survival patterns more accurately. Priority will be placed on minimizing false negatives, as missing actual survivors can be more critical in this context.","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(RF_grid.best_estimator_, X_train, y_train,cv=3)\nprint(\"Cross-validation scores (3-fold) : \",scores)\nprint(\"Average CV accuracy = \",round(scores.mean() ,4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:27.686049Z","iopub.execute_input":"2025-06-29T08:40:27.686312Z","iopub.status.idle":"2025-06-29T08:40:28.595903Z","shell.execute_reply.started":"2025-06-29T08:40:27.686287Z","shell.execute_reply":"2025-06-29T08:40:28.595242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = RF_grid.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T08:40:28.596647Z","iopub.execute_input":"2025-06-29T08:40:28.596910Z","iopub.status.idle":"2025-06-29T08:40:28.630192Z","shell.execute_reply.started":"2025-06-29T08:40:28.596886Z","shell.execute_reply":"2025-06-29T08:40:28.629468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìà 5. Model Evaluation\nTrained the model on preprocessed training data.\n- Achieved validation accuracy of ~80.4%\n- Submitted results to Kaggle and achieved public leaderboard score of 0.78708 (Top - 11%)\n","metadata":{}},{"cell_type":"markdown","source":"# ‚úÖ 6. Final Summary\nIn this project, I built a machine learning model to predict passenger survival on the Titanic dataset. After exploring the data and engineering new features like `Title`, `FamilySize`, and `IsAlone`, I trained a Random Forest classifier to perform the prediction task.\nKey steps:\n- Performed basic data cleaning and handled missing values (e.g., Age, Embarked).\n- Created meaningful features from existing columns to improve prediction power.\n- Trained a Random Forest model using GridSearchCV for hyperparameter tuning to balance bias and variance.\n- Achieved a **public leaderboard score of 0.78708**, placing the model above baseline solutions.\n   \nNext steps to improve the model in future:\n- Add cross-validation (e.g., Stratified K-Fold) to get a more reliable performance estimate.\n- Try other models like XGBoost, Gradient Boosting, or stacking ensembles.\n- Tune hyperparameters using GridSearchCV or RandomizedSearchCV.  \n\n\nThis project demonstrates my ability to apply the full ML pipeline : from data preprocessing and feature engineering to modeling and evaluation : on real-world tabular data.","metadata":{}},{"cell_type":"markdown","source":"Please consider upvoting if you found value from this Notebook!!","metadata":{}}]}